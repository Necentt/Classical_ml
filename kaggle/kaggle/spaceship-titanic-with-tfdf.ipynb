{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "version": "3.6.4",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "name": "python",
   "mimetype": "text/x-python"
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Spaceship Titanic Dataset with TensorFlow Decision Forests"
   ],
   "metadata": {
    "id": "Ck00s7mTmnjA",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This notebook walks you through how to train a baseline Random Forest model using TensorFlow Decision Forests on the Spaceship Titanic dataset made available for this competition.\n",
    "\n",
    "Roughly, the code will look as follows:\n",
    "\n",
    "```\n",
    "import tensorflow_decision_forests as tfdf\n",
    "import pandas as pd\n",
    "\n",
    "dataset = pd.read_csv(\"project/dataset.csv\")\n",
    "tf_dataset = tfdf.keras.pd_dataframe_to_tf_dataset(dataset, label=\"my_label\")\n",
    "\n",
    "model = tfdf.keras.RandomForestModel()\n",
    "model.fit(tf_dataset)\n",
    "\n",
    "print(model.summary())\n",
    "```\n",
    "\n",
    "Decision Forests are a family of tree-based models including Random Forests and Gradient Boosted Trees. They are the best place to start when working with tabular data, and will often outperform (or provide a strong baseline) before you begin experimenting with neural networks."
   ],
   "metadata": {
    "id": "62leGXylmnjF",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Import the library"
   ],
   "metadata": {
    "id": "UPNzfVOEmnjH",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_decision_forests as tfdf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ],
   "metadata": {
    "id": "mmwBzpblmnjH",
    "execution": {
     "iopub.status.busy": "2023-03-17T11:37:48.407982Z",
     "iopub.execute_input": "2023-03-17T11:37:48.408367Z",
     "iopub.status.idle": "2023-03-17T11:38:00.236292Z",
     "shell.execute_reply.started": "2023-03-17T11:37:48.408331Z",
     "shell.execute_reply": "2023-03-17T11:38:00.234875Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Failure to load the inference.so custom c++ tensorflow ops. This error is likely caused the version of TensorFlow and TensorFlow Decision Forests are not compatible. Full error:C:\\Users\\Борис\\PycharmProjects\\Classical_ml\\venv\\Lib\\site-packages\\tensorflow_decision_forests\\tensorflow\\ops\\inference\\inference.so not found\n"
     ]
    },
    {
     "ename": "NotFoundError",
     "evalue": "C:\\Users\\Борис\\PycharmProjects\\Classical_ml\\venv\\Lib\\site-packages\\tensorflow_decision_forests\\tensorflow\\ops\\inference\\inference.so not found",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNotFoundError\u001B[0m                             Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[2], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mtf\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtensorflow_decision_forests\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mtfdf\u001B[39;00m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mpandas\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mpd\u001B[39;00m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mnumpy\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mnp\u001B[39;00m\n",
      "File \u001B[1;32m~\\PycharmProjects\\Classical_ml\\venv\\Lib\\site-packages\\tensorflow_decision_forests\\__init__.py:64\u001B[0m\n\u001B[0;32m     60\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow_decision_forests\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtensorflow\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m check_version\n\u001B[0;32m     62\u001B[0m check_version\u001B[38;5;241m.\u001B[39mcheck_version(__version__, compatible_tf_versions)\n\u001B[1;32m---> 64\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow_decision_forests\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m keras\n\u001B[0;32m     65\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow_decision_forests\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcomponent\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m py_tree\n\u001B[0;32m     66\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow_decision_forests\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcomponent\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mbuilder\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m builder\n",
      "File \u001B[1;32m~\\PycharmProjects\\Classical_ml\\venv\\Lib\\site-packages\\tensorflow_decision_forests\\keras\\__init__.py:53\u001B[0m\n\u001B[0;32m     15\u001B[0m \u001B[38;5;124;03m\"\"\"Decision Forest in a Keras Model.\u001B[39;00m\n\u001B[0;32m     16\u001B[0m \n\u001B[0;32m     17\u001B[0m \u001B[38;5;124;03mUsage example:\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     48\u001B[0m \u001B[38;5;124;03m```\u001B[39;00m\n\u001B[0;32m     49\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m     51\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtyping\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Callable, List\n\u001B[1;32m---> 53\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow_decision_forests\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mkeras\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m core\n\u001B[0;32m     54\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow_decision_forests\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mkeras\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m wrappers\n\u001B[0;32m     56\u001B[0m \u001B[38;5;66;03m# Utility classes\u001B[39;00m\n",
      "File \u001B[1;32m~\\PycharmProjects\\Classical_ml\\venv\\Lib\\site-packages\\tensorflow_decision_forests\\keras\\core.py:62\u001B[0m\n\u001B[0;32m     60\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow_decision_forests\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcomponent\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01minspector\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m inspector \u001B[38;5;28;01mas\u001B[39;00m inspector_lib\n\u001B[0;32m     61\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow_decision_forests\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcomponent\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtuner\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m tuner \u001B[38;5;28;01mas\u001B[39;00m tuner_lib\n\u001B[1;32m---> 62\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow_decision_forests\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mkeras\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m core_inference\n\u001B[0;32m     63\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow_decision_forests\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtensorflow\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m cc_logging\n\u001B[0;32m     64\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow_decision_forests\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtensorflow\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m core \u001B[38;5;28;01mas\u001B[39;00m tf_core\n",
      "File \u001B[1;32m~\\PycharmProjects\\Classical_ml\\venv\\Lib\\site-packages\\tensorflow_decision_forests\\keras\\core_inference.py:36\u001B[0m\n\u001B[0;32m     34\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow_decision_forests\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtensorflow\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m core_inference \u001B[38;5;28;01mas\u001B[39;00m tf_core\n\u001B[0;32m     35\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow_decision_forests\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtensorflow\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m tf_logging\n\u001B[1;32m---> 36\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow_decision_forests\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mops\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01minference\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m api \u001B[38;5;28;01mas\u001B[39;00m tf_op\n\u001B[0;32m     37\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01myggdrasil_decision_forests\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mlearner\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m abstract_learner_pb2\n\u001B[0;32m     38\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01myggdrasil_decision_forests\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mlearner\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmultitasker\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m multitasker_pb2\n",
      "File \u001B[1;32m~\\PycharmProjects\\Classical_ml\\venv\\Lib\\site-packages\\tensorflow_decision_forests\\tensorflow\\ops\\inference\\api.py:179\u001B[0m\n\u001B[0;32m    177\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow_decision_forests\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcomponent\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01minspector\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m inspector \u001B[38;5;28;01mas\u001B[39;00m inspector_lib\n\u001B[0;32m    178\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow_decision_forests\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtensorflow\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m tf1_compatibility\n\u001B[1;32m--> 179\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow_decision_forests\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mops\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01minference\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m op\n\u001B[0;32m    180\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01myggdrasil_decision_forests\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdataset\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m data_spec_pb2\n\u001B[0;32m    181\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01myggdrasil_decision_forests\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmodel\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m abstract_model_pb2\n",
      "File \u001B[1;32m~\\PycharmProjects\\Classical_ml\\venv\\Lib\\site-packages\\tensorflow_decision_forests\\tensorflow\\ops\\inference\\op.py:15\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# Copyright 2021 Google LLC.\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;66;03m#\u001B[39;00m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     12\u001B[0m \u001B[38;5;66;03m# See the License for the specific language governing permissions and\u001B[39;00m\n\u001B[0;32m     13\u001B[0m \u001B[38;5;66;03m# limitations under the License.\u001B[39;00m\n\u001B[1;32m---> 15\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow_decision_forests\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mops\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01minference\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mop_dynamic\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;241m*\u001B[39m\n",
      "File \u001B[1;32m~\\PycharmProjects\\Classical_ml\\venv\\Lib\\site-packages\\tensorflow_decision_forests\\tensorflow\\ops\\inference\\op_dynamic.py:24\u001B[0m\n\u001B[0;32m     22\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m     23\u001B[0m   check_version\u001B[38;5;241m.\u001B[39minfo_fail_to_load_custom_op(e, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minference.so\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m---> 24\u001B[0m   \u001B[38;5;28;01mraise\u001B[39;00m e\n\u001B[0;32m     26\u001B[0m \u001B[38;5;66;03m# Importing all the symbols.\u001B[39;00m\n\u001B[0;32m     27\u001B[0m module \u001B[38;5;241m=\u001B[39m sys\u001B[38;5;241m.\u001B[39mmodules[\u001B[38;5;18m__name__\u001B[39m]\n",
      "File \u001B[1;32m~\\PycharmProjects\\Classical_ml\\venv\\Lib\\site-packages\\tensorflow_decision_forests\\tensorflow\\ops\\inference\\op_dynamic.py:21\u001B[0m\n\u001B[0;32m     18\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01msys\u001B[39;00m\n\u001B[0;32m     20\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m---> 21\u001B[0m   ops \u001B[38;5;241m=\u001B[39m \u001B[43mtf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload_op_library\u001B[49m\u001B[43m(\u001B[49m\u001B[43mresource_loader\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_path_to_datafile\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43minference.so\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     22\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m     23\u001B[0m   check_version\u001B[38;5;241m.\u001B[39minfo_fail_to_load_custom_op(e, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minference.so\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32m~\\PycharmProjects\\Classical_ml\\venv\\Lib\\site-packages\\tensorflow\\python\\framework\\load_library.py:54\u001B[0m, in \u001B[0;36mload_op_library\u001B[1;34m(library_filename)\u001B[0m\n\u001B[0;32m     31\u001B[0m \u001B[38;5;129m@tf_export\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mload_op_library\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m     32\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mload_op_library\u001B[39m(library_filename):\n\u001B[0;32m     33\u001B[0m \u001B[38;5;250m  \u001B[39m\u001B[38;5;124;03m\"\"\"Loads a TensorFlow plugin, containing custom ops and kernels.\u001B[39;00m\n\u001B[0;32m     34\u001B[0m \n\u001B[0;32m     35\u001B[0m \u001B[38;5;124;03m  Pass \"library_filename\" to a platform-specific mechanism for dynamically\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     52\u001B[0m \u001B[38;5;124;03m    RuntimeError: when unable to load the library or get the python wrappers.\u001B[39;00m\n\u001B[0;32m     53\u001B[0m \u001B[38;5;124;03m  \"\"\"\u001B[39;00m\n\u001B[1;32m---> 54\u001B[0m   lib_handle \u001B[38;5;241m=\u001B[39m \u001B[43mpy_tf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mTF_LoadLibrary\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlibrary_filename\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     55\u001B[0m   \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m     56\u001B[0m     wrappers \u001B[38;5;241m=\u001B[39m _pywrap_python_op_gen\u001B[38;5;241m.\u001B[39mGetPythonWrappers(\n\u001B[0;32m     57\u001B[0m         py_tf\u001B[38;5;241m.\u001B[39mTF_GetOpList(lib_handle))\n",
      "\u001B[1;31mNotFoundError\u001B[0m: C:\\Users\\Борис\\PycharmProjects\\Classical_ml\\venv\\Lib\\site-packages\\tensorflow_decision_forests\\tensorflow\\ops\\inference\\inference.so not found"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "print(\"TensorFlow v\" + tf.__version__)\n",
    "print(\"TensorFlow Decision Forests v\" + tfdf.__version__)"
   ],
   "metadata": {
    "id": "grAO86gTmnjJ",
    "execution": {
     "iopub.status.busy": "2023-03-17T11:38:00.238349Z",
     "iopub.execute_input": "2023-03-17T11:38:00.239383Z",
     "iopub.status.idle": "2023-03-17T11:38:00.246806Z",
     "shell.execute_reply.started": "2023-03-17T11:38:00.239340Z",
     "shell.execute_reply": "2023-03-17T11:38:00.245563Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load the Dataset"
   ],
   "metadata": {
    "id": "6sHFpppPmnjJ",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Load a dataset into a Pandas Dataframe\n",
    "dataset_df = pd.read_csv('/kaggle/input/spaceship-titanic/train.csv')\n",
    "print(\"Full train dataset shape is {}\".format(dataset_df.shape))"
   ],
   "metadata": {
    "id": "c1P3Y3a7mnjL",
    "execution": {
     "iopub.status.busy": "2023-03-17T11:38:00.248430Z",
     "iopub.execute_input": "2023-03-17T11:38:00.248845Z",
     "iopub.status.idle": "2023-03-17T11:38:00.331706Z",
     "shell.execute_reply.started": "2023-03-17T11:38:00.248794Z",
     "shell.execute_reply": "2023-03-17T11:38:00.330322Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "The data is composed of 14 columns and 8693 entries. We can see all 14 dimensions of our dataset by printing out the first 5 entries using the following code:"
   ],
   "metadata": {
    "id": "cEd92zhJmnjL",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Display the first 5 examples\n",
    "dataset_df.head(5)"
   ],
   "metadata": {
    "id": "nCx3PE1xmnjM",
    "execution": {
     "iopub.status.busy": "2023-03-17T11:38:00.334527Z",
     "iopub.execute_input": "2023-03-17T11:38:00.334921Z",
     "iopub.status.idle": "2023-03-17T11:38:00.375222Z",
     "shell.execute_reply.started": "2023-03-17T11:38:00.334887Z",
     "shell.execute_reply": "2023-03-17T11:38:00.373944Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "There are 12 feature columns. Using these features your model has to predict whether the passenger is rescued or not indicated by the column `Transported`."
   ],
   "metadata": {
    "id": "0-Euaq6dmnjN",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Let us quickly do a basic exploration of the dataset"
   ],
   "metadata": {
    "id": "1-Ewr6XDmnjN",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "dataset_df.describe()"
   ],
   "metadata": {
    "id": "XjwG5wjfmnjO",
    "execution": {
     "iopub.status.busy": "2023-03-17T11:38:00.377568Z",
     "iopub.execute_input": "2023-03-17T11:38:00.378197Z",
     "iopub.status.idle": "2023-03-17T11:38:00.424386Z",
     "shell.execute_reply.started": "2023-03-17T11:38:00.378158Z",
     "shell.execute_reply": "2023-03-17T11:38:00.422951Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "dataset_df.info()"
   ],
   "metadata": {
    "id": "UmWpnVxQmnjO",
    "execution": {
     "iopub.status.busy": "2023-03-17T11:38:00.426217Z",
     "iopub.execute_input": "2023-03-17T11:38:00.426812Z",
     "iopub.status.idle": "2023-03-17T11:38:00.453019Z",
     "shell.execute_reply.started": "2023-03-17T11:38:00.426758Z",
     "shell.execute_reply": "2023-03-17T11:38:00.451960Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Bar chart for label column: Transported\n",
    "\n"
   ],
   "metadata": {
    "id": "PYbIPVaCmnjO",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "plot_df = dataset_df.Transported.value_counts()\n",
    "plot_df.plot(kind=\"bar\")"
   ],
   "metadata": {
    "id": "DcaGweARmnjP",
    "execution": {
     "iopub.status.busy": "2023-03-17T11:38:00.455165Z",
     "iopub.execute_input": "2023-03-17T11:38:00.455536Z",
     "iopub.status.idle": "2023-03-17T11:38:00.942523Z",
     "shell.execute_reply.started": "2023-03-17T11:38:00.455487Z",
     "shell.execute_reply": "2023-03-17T11:38:00.941488Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Numerical data distribution\n",
    "\n",
    "Let us plot all the numerical columns and their value counts:"
   ],
   "metadata": {
    "id": "LRO2hJlNmnjP",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "fig, ax = plt.subplots(5,1,  figsize=(10, 10))\n",
    "plt.subplots_adjust(top = 2)\n",
    "\n",
    "sns.histplot(dataset_df['Age'], color='b', bins=50, ax=ax[0]);\n",
    "sns.histplot(dataset_df['FoodCourt'], color='b', bins=50, ax=ax[1]);\n",
    "sns.histplot(dataset_df['ShoppingMall'], color='b', bins=50, ax=ax[2]);\n",
    "sns.histplot(dataset_df['Spa'], color='b', bins=50, ax=ax[3]);\n",
    "sns.histplot(dataset_df['VRDeck'], color='b', bins=50, ax=ax[4]);"
   ],
   "metadata": {
    "id": "lafxj4fkmnjQ",
    "execution": {
     "iopub.status.busy": "2023-03-17T11:38:00.943650Z",
     "iopub.execute_input": "2023-03-17T11:38:00.944560Z",
     "iopub.status.idle": "2023-03-17T11:38:02.402936Z",
     "shell.execute_reply.started": "2023-03-17T11:38:00.944520Z",
     "shell.execute_reply": "2023-03-17T11:38:02.401861Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Prepare the dataset"
   ],
   "metadata": {
    "id": "Trlsxv1emnjQ",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We will drop both `PassengerId` and `Name` columns as they are not necessary for model training."
   ],
   "metadata": {
    "id": "c-BLN1jcmnjQ",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "dataset_df = dataset_df.drop(['PassengerId', 'Name'], axis=1)\n",
    "dataset_df.head(5)"
   ],
   "metadata": {
    "id": "nftfBG67mnjR",
    "execution": {
     "iopub.status.busy": "2023-03-17T11:38:02.404126Z",
     "iopub.execute_input": "2023-03-17T11:38:02.404916Z",
     "iopub.status.idle": "2023-03-17T11:38:02.427789Z",
     "shell.execute_reply.started": "2023-03-17T11:38:02.404878Z",
     "shell.execute_reply": "2023-03-17T11:38:02.426792Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "We will check for the missing values using the following code:"
   ],
   "metadata": {
    "id": "pXhbIKCemnjR",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "dataset_df.isnull().sum().sort_values(ascending=False)"
   ],
   "metadata": {
    "id": "3_VMZ9a1mnjR",
    "execution": {
     "iopub.status.busy": "2023-03-17T11:38:02.431608Z",
     "iopub.execute_input": "2023-03-17T11:38:02.432564Z",
     "iopub.status.idle": "2023-03-17T11:38:02.451371Z",
     "shell.execute_reply.started": "2023-03-17T11:38:02.432521Z",
     "shell.execute_reply": "2023-03-17T11:38:02.449797Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "This dataset contains a mix of numeric, categorical and missing features. TF-DF supports all these feature types natively, and no preprocessing is required.\n",
    "\n",
    "But this datatset also has boolean fields with missing values. TF-DF doesn't support boolean fields yet. So we need to convert those fields into int. To account for the missing values in the boolean fields, we will replace them with zero.\n",
    "\n",
    "In this notebook, we will replace null value entries with zero for numerical columns as well and only let TF-DF handle the missing values in categorical columns.\n",
    "\n",
    "Note: You can choose to let TF-DF handle missing values in numerical columns if need be."
   ],
   "metadata": {
    "id": "vlWOQl4fmnjS",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "dataset_df[['VIP', 'CryoSleep', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']] = dataset_df[['VIP', 'CryoSleep', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']].fillna(value=0)\n",
    "dataset_df.isnull().sum().sort_values(ascending=False)"
   ],
   "metadata": {
    "id": "ASQ5Bh2JmnjS",
    "execution": {
     "iopub.status.busy": "2023-03-17T11:38:02.452817Z",
     "iopub.execute_input": "2023-03-17T11:38:02.453226Z",
     "iopub.status.idle": "2023-03-17T11:38:02.474601Z",
     "shell.execute_reply.started": "2023-03-17T11:38:02.453184Z",
     "shell.execute_reply": "2023-03-17T11:38:02.473150Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Since, TF-DF cannot handle boolean columns, we will have to adjust the labels in column `Transported` to convert them into the integer format that TF-DF expects."
   ],
   "metadata": {
    "id": "tFbE6D0BmnjT",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "label = \"Transported\"\n",
    "dataset_df[label] = dataset_df[label].astype(int)"
   ],
   "metadata": {
    "id": "-T6a_RTxmnjT",
    "execution": {
     "iopub.status.busy": "2023-03-17T11:38:02.476304Z",
     "iopub.execute_input": "2023-03-17T11:38:02.476771Z",
     "iopub.status.idle": "2023-03-17T11:38:02.483629Z",
     "shell.execute_reply.started": "2023-03-17T11:38:02.476723Z",
     "shell.execute_reply": "2023-03-17T11:38:02.482480Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "We will also convert the boolean fields `CryoSleep` and `VIP` to int."
   ],
   "metadata": {
    "id": "F6-GATuzmnjU",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "dataset_df['VIP'] = dataset_df['VIP'].astype(int)\n",
    "dataset_df['CryoSleep'] = dataset_df['CryoSleep'].astype(int)"
   ],
   "metadata": {
    "id": "Wq6PbMnnmnjU",
    "execution": {
     "iopub.status.busy": "2023-03-17T11:38:02.485280Z",
     "iopub.execute_input": "2023-03-17T11:38:02.486082Z",
     "iopub.status.idle": "2023-03-17T11:38:02.500728Z",
     "shell.execute_reply.started": "2023-03-17T11:38:02.486027Z",
     "shell.execute_reply": "2023-03-17T11:38:02.499677Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "The value of column `Cabin` is a string with the format `Deck/Cabin_num/Side`. Here we will split the `Cabin` column and create 3 new columns `Deck`, `Cabin_num` and `Side`, since it will be easier to train the model on those individual data.\n",
    "\n",
    "Run the following command to split the column `Cabin` into columns `Deck`, `Cabin_num` and `Side`"
   ],
   "metadata": {
    "id": "enjp51h_mnjV",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "dataset_df[[\"Deck\", \"Cabin_num\", \"Side\"]] = dataset_df[\"Cabin\"].str.split(\"/\", expand=True)"
   ],
   "metadata": {
    "id": "JeAuUvb_mnjV",
    "execution": {
     "iopub.status.busy": "2023-03-17T11:38:02.502051Z",
     "iopub.execute_input": "2023-03-17T11:38:02.502591Z",
     "iopub.status.idle": "2023-03-17T11:38:02.532257Z",
     "shell.execute_reply.started": "2023-03-17T11:38:02.502556Z",
     "shell.execute_reply": "2023-03-17T11:38:02.530824Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Remove original `Cabin` column from the dataset since it's not needed anymore."
   ],
   "metadata": {
    "id": "vNYEd1AkmnjV",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "try:\n",
    "    dataset_df = dataset_df.drop('Cabin', axis=1)\n",
    "except KeyError:\n",
    "    print(\"Field does not exist\")"
   ],
   "metadata": {
    "id": "dOcny3s4mnjV",
    "execution": {
     "iopub.status.busy": "2023-03-17T11:38:02.534001Z",
     "iopub.execute_input": "2023-03-17T11:38:02.534808Z",
     "iopub.status.idle": "2023-03-17T11:38:02.545112Z",
     "shell.execute_reply.started": "2023-03-17T11:38:02.534733Z",
     "shell.execute_reply": "2023-03-17T11:38:02.543488Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let us display the first 5 examples from the prepared dataset."
   ],
   "metadata": {
    "id": "Y7byxj5FmnjW",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "dataset_df.head(5)"
   ],
   "metadata": {
    "id": "g0U1OKAYmnjW",
    "execution": {
     "iopub.status.busy": "2023-03-17T11:38:02.546894Z",
     "iopub.execute_input": "2023-03-17T11:38:02.547460Z",
     "iopub.status.idle": "2023-03-17T11:38:02.570683Z",
     "shell.execute_reply.started": "2023-03-17T11:38:02.547424Z",
     "shell.execute_reply": "2023-03-17T11:38:02.569490Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now let us split the dataset into training and testing datasets:"
   ],
   "metadata": {
    "id": "l6EmJsSfmnjW",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def split_dataset(dataset, test_ratio=0.20):\n",
    "  test_indices = np.random.rand(len(dataset)) < test_ratio\n",
    "  return dataset[~test_indices], dataset[test_indices]\n",
    "\n",
    "train_ds_pd, valid_ds_pd = split_dataset(dataset_df)\n",
    "print(\"{} examples in training, {} examples in testing.\".format(\n",
    "    len(train_ds_pd), len(valid_ds_pd)))"
   ],
   "metadata": {
    "id": "_drMgwAAmnjX",
    "execution": {
     "iopub.status.busy": "2023-03-17T11:38:02.572901Z",
     "iopub.execute_input": "2023-03-17T11:38:02.573420Z",
     "iopub.status.idle": "2023-03-17T11:38:02.589814Z",
     "shell.execute_reply.started": "2023-03-17T11:38:02.573367Z",
     "shell.execute_reply": "2023-03-17T11:38:02.588363Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "There's one more step required before we can train the model. We need to convert the datatset from Pandas format (`pd.DataFrame`) into TensorFlow Datasets format (`tf.data.Dataset`).\n",
    "\n",
    "[TensorFlow Datasets](https://www.tensorflow.org/datasets/overview) is a high performance data loading library which is helpful when training neural networks with accelerators like GPUs and TPUs."
   ],
   "metadata": {
    "id": "_gg4nfB1mnjX",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "train_ds = tfdf.keras.pd_dataframe_to_tf_dataset(train_ds_pd, label=label)\n",
    "valid_ds = tfdf.keras.pd_dataframe_to_tf_dataset(valid_ds_pd, label=label)"
   ],
   "metadata": {
    "id": "LZADOplXmnjX",
    "execution": {
     "iopub.status.busy": "2023-03-17T11:38:02.591068Z",
     "iopub.execute_input": "2023-03-17T11:38:02.591679Z",
     "iopub.status.idle": "2023-03-17T11:38:02.784788Z",
     "shell.execute_reply.started": "2023-03-17T11:38:02.591640Z",
     "shell.execute_reply": "2023-03-17T11:38:02.781648Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Select a Model\n",
    "\n",
    "There are several tree-based models for you to choose from.\n",
    "\n",
    "* RandomForestModel\n",
    "* GradientBoostedTreesModel\n",
    "* CartModel\n",
    "* DistributedGradientBoostedTreesModel\n",
    "\n",
    "To start, we'll work with a Random Forest. This is the most well-known of the Decision Forest training algorithms.\n",
    "\n",
    "A Random Forest is a collection of decision trees, each trained independently on a random subset of the training dataset (sampled with replacement). The algorithm is unique in that it is robust to overfitting, and easy to use.\n",
    "\n",
    "We can list the all the available models in TensorFlow Decision Forests using the following code:"
   ],
   "metadata": {
    "id": "0-xUiOTwmnjY",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "tfdf.keras.get_all_models()"
   ],
   "metadata": {
    "id": "PdQw1cWbmnjY",
    "execution": {
     "iopub.status.busy": "2023-03-17T11:38:02.786719Z",
     "iopub.execute_input": "2023-03-17T11:38:02.787233Z",
     "iopub.status.idle": "2023-03-17T11:38:02.801533Z",
     "shell.execute_reply.started": "2023-03-17T11:38:02.787177Z",
     "shell.execute_reply": "2023-03-17T11:38:02.798320Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Configure the model\n",
    "\n",
    "TensorFlow Decision Forests provides good defaults for you (e.g. the top ranking hyperparameters on our benchmarks, slightly modified to run in reasonable time). If you would like to configure the learning algorithm, you will find many options you can explore to get the highest possible accuracy.\n",
    "\n",
    "You can select a template and/or set parameters as follows:\n",
    "\n",
    "```rf = tfdf.keras.RandomForestModel(hyperparameter_template=\"benchmark_rank1\")```\n",
    "\n",
    "Read more [here](https://www.tensorflow.org/decision_forests/api_docs/python/tfdf/keras/RandomForestModel)."
   ],
   "metadata": {
    "id": "sy81fpfxmnjY",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Create a Random Forest\n",
    "\n",
    "Today, we will use the defaults to create the Random Forest Model. By default the model is set to train for a classification task."
   ],
   "metadata": {
    "id": "AfJMCA6lmnjZ",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "rf = tfdf.keras.RandomForestModel()\n",
    "rf.compile(metrics=[\"accuracy\"]) # Optional, you can use this to include a list of eval metrics"
   ],
   "metadata": {
    "id": "j7-gFVDNmnjZ",
    "execution": {
     "iopub.status.busy": "2023-03-17T11:38:02.803664Z",
     "iopub.execute_input": "2023-03-17T11:38:02.805636Z",
     "iopub.status.idle": "2023-03-17T11:38:02.880529Z",
     "shell.execute_reply.started": "2023-03-17T11:38:02.805571Z",
     "shell.execute_reply": "2023-03-17T11:38:02.879000Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Train the model\n",
    "\n",
    "We will train the model using a one-liner.\n",
    "\n",
    "Note: you may see a warning about Autograph. You can safely ignore this, it will be fixed in the next release."
   ],
   "metadata": {
    "id": "wEQXtv4MmnjZ",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "rf.fit(x=train_ds)"
   ],
   "metadata": {
    "id": "QcL5KRyLmnja",
    "execution": {
     "iopub.status.busy": "2023-03-17T11:38:02.882110Z",
     "iopub.execute_input": "2023-03-17T11:38:02.883042Z",
     "iopub.status.idle": "2023-03-17T11:39:06.645138Z",
     "shell.execute_reply.started": "2023-03-17T11:38:02.882994Z",
     "shell.execute_reply": "2023-03-17T11:39:06.644102Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Visualize the model\n",
    "One benefit of tree-based models is that we can easily visualize them. The default number of trees used in the Random Forests is 300. We can select a tree to display below."
   ],
   "metadata": {
    "id": "HQaPDLBjmnja",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "tfdf.model_plotter.plot_model_in_colab(rf, tree_idx=0, max_depth=3)"
   ],
   "metadata": {
    "id": "hSao4Qstmnja",
    "execution": {
     "iopub.status.busy": "2023-03-17T11:39:06.646833Z",
     "iopub.execute_input": "2023-03-17T11:39:06.647203Z",
     "iopub.status.idle": "2023-03-17T11:39:07.196444Z",
     "shell.execute_reply.started": "2023-03-17T11:39:06.647167Z",
     "shell.execute_reply": "2023-03-17T11:39:07.195239Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Evaluate the model on the Out of bag (OOB) data and the validation dataset\n",
    "\n",
    "Before training the dataset we have manually seperated 20% of the dataset for validation named as `valid_ds`.\n",
    "\n",
    "We can also use Out of bag (OOB) score to validate our RandomForestModel.\n",
    "To train a Random Forest Model, a set of random samples from training set are choosen by the algorithm and the rest of the samples are used to finetune the model.The subset of data that is not chosen is known as Out of bag data (OOB).\n",
    "OOB score is computed on the OOB data.\n",
    "\n",
    "Read more about OOB data [here](https://developers.google.com/machine-learning/decision-forests/out-of-bag).\n",
    "\n",
    "The training logs show the accuracy evaluated on the out-of-bag dataset according to the number of trees in the model. Let us plot this.\n",
    "\n",
    "Note: Larger values are better for this hyperparameter."
   ],
   "metadata": {
    "id": "1q5zxzuLmnjb",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "logs = rf.make_inspector().training_logs()\n",
    "plt.plot([log.num_trees for log in logs], [log.evaluation.accuracy for log in logs])\n",
    "plt.xlabel(\"Number of trees\")\n",
    "plt.ylabel(\"Accuracy (out-of-bag)\")\n",
    "plt.show()"
   ],
   "metadata": {
    "id": "z7u4IuAQmnjb",
    "execution": {
     "iopub.status.busy": "2023-03-17T11:39:07.197804Z",
     "iopub.execute_input": "2023-03-17T11:39:07.198663Z",
     "iopub.status.idle": "2023-03-17T11:39:07.422948Z",
     "shell.execute_reply.started": "2023-03-17T11:39:07.198625Z",
     "shell.execute_reply": "2023-03-17T11:39:07.421611Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can also see some general stats on the OOB dataset:"
   ],
   "metadata": {
    "id": "N24V7vhAmnjb",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "inspector = rf.make_inspector()\n",
    "inspector.evaluation()"
   ],
   "metadata": {
    "id": "QTD92zMGmnjc",
    "execution": {
     "iopub.status.busy": "2023-03-17T11:39:07.424578Z",
     "iopub.execute_input": "2023-03-17T11:39:07.425140Z",
     "iopub.status.idle": "2023-03-17T11:39:07.434692Z",
     "shell.execute_reply.started": "2023-03-17T11:39:07.425104Z",
     "shell.execute_reply": "2023-03-17T11:39:07.433349Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, let us run an evaluation using the validation dataset."
   ],
   "metadata": {
    "id": "SuVS0BGXmnjd",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "evaluation = rf.evaluate(x=valid_ds,return_dict=True)\n",
    "\n",
    "for name, value in evaluation.items():\n",
    "  print(f\"{name}: {value:.4f}\")"
   ],
   "metadata": {
    "id": "C0SfSxc0mnjd",
    "execution": {
     "iopub.status.busy": "2023-03-17T11:39:07.436280Z",
     "iopub.execute_input": "2023-03-17T11:39:07.436700Z",
     "iopub.status.idle": "2023-03-17T11:39:08.066244Z",
     "shell.execute_reply.started": "2023-03-17T11:39:07.436667Z",
     "shell.execute_reply": "2023-03-17T11:39:08.065264Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Variable importances\n",
    "\n",
    "Variable importances generally indicate how much a feature contributes to the model predictions or quality. There are several ways to identify important features using TensorFlow Decision Forests.\n",
    "Let us list the available `Variable Importances` for Decision Trees:"
   ],
   "metadata": {
    "id": "cgAI1XrVmnje",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "print(f\"Available variable importances:\")\n",
    "for importance in inspector.variable_importances().keys():\n",
    "  print(\"\\t\", importance)"
   ],
   "metadata": {
    "id": "BdPN_fV1mnje",
    "execution": {
     "iopub.status.busy": "2023-03-17T11:39:08.067559Z",
     "iopub.execute_input": "2023-03-17T11:39:08.068122Z",
     "iopub.status.idle": "2023-03-17T11:39:08.073989Z",
     "shell.execute_reply.started": "2023-03-17T11:39:08.068087Z",
     "shell.execute_reply": "2023-03-17T11:39:08.072841Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "As an example, let us display the important features for the Variable Importance `NUM_AS_ROOT`.\n",
    "\n",
    "The larger the importance score for `NUM_AS_ROOT`, the more impact it has on the outcome of the model.\n",
    "\n",
    "By default, the list is sorted from the most important to the least. From the output you can infer that the feature at the top of the list is used as the root node in most number of trees in the random forest than any other feature."
   ],
   "metadata": {
    "id": "dr-nVQwCmnje",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Each line is: (feature name, (index of the feature), importance score)\n",
    "inspector.variable_importances()[\"NUM_AS_ROOT\"]"
   ],
   "metadata": {
    "id": "etYOifhfmnje",
    "execution": {
     "iopub.status.busy": "2023-03-17T11:39:08.075488Z",
     "iopub.execute_input": "2023-03-17T11:39:08.076101Z",
     "iopub.status.idle": "2023-03-17T11:39:08.091160Z",
     "shell.execute_reply.started": "2023-03-17T11:39:08.076064Z",
     "shell.execute_reply": "2023-03-17T11:39:08.089811Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Submission"
   ],
   "metadata": {
    "id": "DAtAR0vkmnje",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Load the test dataset\n",
    "test_df = pd.read_csv('/kaggle/input/spaceship-titanic/test.csv')\n",
    "submission_id = test_df.PassengerId\n",
    "\n",
    "# Replace NaN values with zero\n",
    "test_df[['VIP', 'CryoSleep']] = test_df[['VIP', 'CryoSleep']].fillna(value=0)\n",
    "\n",
    "# Creating New Features - Deck, Cabin_num and Side from the column Cabin and remove Cabin\n",
    "test_df[[\"Deck\", \"Cabin_num\", \"Side\"]] = test_df[\"Cabin\"].str.split(\"/\", expand=True)\n",
    "test_df = test_df.drop('Cabin', axis=1)\n",
    "\n",
    "# Convert boolean to 1's and 0's\n",
    "test_df['VIP'] = test_df['VIP'].astype(int)\n",
    "test_df['CryoSleep'] = test_df['CryoSleep'].astype(int)\n",
    "\n",
    "# Convert pd dataframe to tf dataset\n",
    "test_ds = tfdf.keras.pd_dataframe_to_tf_dataset(test_df)\n",
    "\n",
    "# Get the predictions for testdata\n",
    "predictions = rf.predict(test_ds)\n",
    "n_predictions = (predictions > 0.5).astype(bool)\n",
    "output = pd.DataFrame({'PassengerId': submission_id,\n",
    "                       'Transported': n_predictions.squeeze()})\n",
    "\n",
    "output.head()"
   ],
   "metadata": {
    "id": "KHBRzBD9mnjf",
    "execution": {
     "iopub.status.busy": "2023-03-17T11:39:08.096864Z",
     "iopub.execute_input": "2023-03-17T11:39:08.097594Z",
     "iopub.status.idle": "2023-03-17T11:39:08.764205Z",
     "shell.execute_reply.started": "2023-03-17T11:39:08.097552Z",
     "shell.execute_reply": "2023-03-17T11:39:08.762812Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "sample_submission_df = pd.read_csv('/kaggle/input/spaceship-titanic/sample_submission.csv')\n",
    "sample_submission_df['Transported'] = n_predictions\n",
    "sample_submission_df.to_csv('/kaggle/working/submission.csv', index=False)\n",
    "sample_submission_df.head()"
   ],
   "metadata": {
    "id": "OZuB6CdUmnjf",
    "execution": {
     "iopub.status.busy": "2023-03-17T11:39:08.766214Z",
     "iopub.execute_input": "2023-03-17T11:39:08.767001Z",
     "iopub.status.idle": "2023-03-17T11:39:08.809562Z",
     "shell.execute_reply.started": "2023-03-17T11:39:08.766942Z",
     "shell.execute_reply": "2023-03-17T11:39:08.807867Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}